<div id="mydiv" style="font-size:50px; font-family: times, serif;">PLUGS &nbsp; &nbsp;<a style="font-size:30px">version 1.0.0</a></div>
<h4>Planning Urban Growth & Simulation : AI for urban policy & form generation</h4>
<div style="font-size:15px; font-color:rgb(100,100,100);">
  <br>student: Nirvik Saha (GIT)
  <br>adviser: Dennis R Shelden (GIT)
  <br>adviser: John R Haymaker (P+W)
  <br>adviser: Perry Pei-Ju Yang (GIT)
  <br>contact-email: nirviksaha@gatech.edu
</div>

</br></br>
<div style="font-size:15px; font-style:italic;">Cities are complex systems whose infrastructural, economic and social components are strongly interrelated and therefore difficult to understand in isolation. The many problems associated with urban growth and global sustainability, however, are typically treated as independent issues. This frequently results in ineffective policy and often leads to unfortunate and sometimes disastrous unintended consequences. Policies meant to control population movements and the spread of slums in megacities, or to reverse urban decay, have largely proven ineffective or counterproductive, despite huge expenditure.</div>
- Bettencourt, Lu√≠s & West, Geoffrey. (2010). A unified theory of urban living. Nature. 467. 912-3. 
</p>
</br>
<p>
This research explores Planning Urban Growth Simulator (PLUGS), a computational methodology to generate design strategy from physical constraints of the region, design primitives or moves and desired attributes. PLUGS assists in design and decision-making by virtually simulating the urban design process and learning from the results to find an optimized solution based on an user-defined objective function. Formulation of the problem and a computational solution relies on reinforcement learning, specifically, deep Q-learning and epsilon-greedy approach. The AI platform for urban design and planning demonstrates placement of known buildings and activities as well as generating building geometry based on urban policy. It demonstrates that a common methodology can be applied to generate rulesets and process them to automate large-scale urban planning problems. The output is in the form of geometric objects which can be evaluated for daylighting, energy consumption, heat island effect and other performance criteria.
</p>
</br>
<p>
An interactive graphical user interface allows the user to input numerical or boolean fields. The inputs received from the interface are coefficients of various attributes of the system. The user may dynamically alter the objective function at runtime by manipulating these attributes. The internal mechanism generates a policy or ruleset for plotting buildings. Once a policy is generated, a procedural model is developed using computational geometry algorithms, Q-learning and epsilon-greedy. The paper provides a comprehensive explanation of the proposed processes, domains of utilization,scope for generalization, and illustration of results.
</p>
</br>
<p>
Using urban simulation, a strategy is projected to the virtual environment by an agent which allows a planner to study the behavior of the model with respect to a range of constraints. Various design moves are cast into a Markov Decision Process and traversed by an agent using Markov chain Monte Carlo process. Each traversal provides a set of states or design moves that is interpreted as a strategy (ruleset) for placing building types and their activities. The concept of discounted rewards is used to balance exploration of the possible solutions against exploitation of the easiest available alternative to accumulate rewards. An important feature of this technique is that extensive data collection is not required to develop strategies because it relies on a reward-maximization policy to iteratively guide the agent. It effectively simulates the data and learns from past iterations. It identifies ideal planning scenarios to guide given a user defined objective function by exposing coefficients of various parameters through a GUI. Since the states of the MDP are simply an abstract data type, various types of constraints can be integrated into the simulation by positioning them appropriately in the MDP. This allows a generalization to other contexts by suitably altering constraints and updating the reward allocation module. This learning mechanism demonstrated is a highly-scalable alternative to usual optimization methods involving heuristics or numerical programming. 
</p>
</br>

</br></br>
<div align="center" style="font-size:25px; font-family: times, serif;">Circulation</div>
<div align="center">
<img src="/public/imgs/plugsAbout02.jpg" alt="Smiley face" height="400" width="800">
</div>
</br></br>
<div align="center" style="font-size:25px; font-family: times, serif;">Activity and Space Allocation</div>
<div align="center">
<img src="/public/imgs/plugsAbout01.jpg" alt="Smiley face" height="400" width="800">
</div>

</br></br>